{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4gf9-d8smV0"
   },
   "source": [
    "## Contenu\n",
    "#### [1. PART one:](#1)\n",
    "- [1.1 Preparation de donn√©es:](#1.1)\n",
    "- [1.2 L'entrainement de mod√©le:](#1.2)\n",
    "- [1.3 La pr√©diction de mod√©le:](#1.3)\n",
    "\n",
    "#### [2. PART two:](#2)\n",
    "- [2.1 Le preprocessing:](#2.1)\n",
    "- [2.1 La g√®n√®ration du texte:](#2.2)\n",
    "- [2.1 L' autocompletion de texte :](#2.3)\n",
    "\n",
    "#### [3. PART tree:](#3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2w6BG1tsvIt"
   },
   "source": [
    "<a name='1'></a>\n",
    "## **I. PART one:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bY0D9QLOttvf"
   },
   "source": [
    "<a name='1.1'></a>\n",
    "1. La methode prepare_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "As7cY92l7mmZ",
    "outputId": "f019fc22-bf5d-42e5-913a-2abbd0f56e0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FHuEPA3mvUmK",
    "outputId": "510e72ca-f619-4777-8fe3-8bbf7e9de1a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting textsearch>=0.0.21 (from contractions)\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
      "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
     ]
    }
   ],
   "source": [
    "# Telecharger le module de contractions\n",
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k4l6dmCAvTBj",
    "outputId": "cf64b1ff-d386-4e33-bd37-37b55c164685"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importer les modules\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import contractions\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk.corpus import words as nltk_words\n",
    "import math\n",
    "import pandas as pd\n",
    "import threading\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a_eMqvTa3jjC",
    "outputId": "02061bcd-8d11-4384-ffba-68940514908f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed corpus: <s>  how are you ? by the way thanks for the <UNK> . you going to be in <UNK> <UNK> soon ? love to see you . been way , way too long .  </s> <s>  when you meet someone special ... you will know . your heart will beat more rapidly and you will smile for no reason .  </s> <s>  they have decided its more fun if i do not .  </s> <s>  so tired d ; <UNK> <UNK> tag & ran a lot d ; <UNK> going to sleep like in 5 <UNK> ; )  </s> <s>  <UNK> from a complete stranger ! made my birthday even better : )  </s> <s>  first <UNK> game ever ! <UNK> field is gorgeous . this is perfect . go <UNK> go !  </s> <s>  i no ! i get another day off from <UNK> due to the wonderful snow ( : and this wakes me up ... damn thing  </s> <s>  i am coo ... <UNK> at work <UNK> tired r you ever in <UNK>  </s> <s>  the new <UNK> commercial ... hehe love at first sight  </s> <s>  we need to reconnect this week  </s> <s>  i always wonder how the <UNK> on the <UNK> <UNK> learned to talk so fast ! ? all i hear is <UNK> .  </s> <s>  <UNK> what a catch  </s> <s>  such a great picture ! the green shirt totally <UNK> out your <UNK> !  </s> <s>  desk put together , room all set up . oh boy , oh boy  </s> <s>  i am doing it ! üë¶  </s> <s>  beauty <UNK> in the alchemy office with and sally walker !  </s> <s>  looking for a new band to <UNK> for the month . anyone interested ?  </s> <s>  <UNK> for a quick move down the street ... if only i had some <UNK> ...  </s> <s>  ford focus <UNK> ?  </s> <s>  <UNK> : according to the national retail federation $ 16.3 billion was spent on # <UNK> last year ! !  </s> <s>  ‚Äú : `` the tragedy of life is not that it <UNK> so soon , but that we wait so long to begin it . '' - w.m . lewis ‚Äù  </s> <s>  more skating ! come by the check out a movie , eat a great dinner and top it off with great times at the ice rink .  </s> <s>  watch your mailbox ! : )  </s>\n",
      "vocab: {'birthday', 'my', 'wonderful', 'your', 'sleep', 'to', 'quick', 'at', 'hear', 'down', 'spent', 'heart', 'that', 'i', 'ran', 'put', 'lot', 'retail', 'even', 'but', 'oh', 'learned', 'such', 'interested', 'not', 'what', 'someone', 'band', 'thanks', 'r', 'doing', 'this', 'off', 'coo', 'was', 'too', 'first', 'always', 'like', 'a', 'you', 'skating', 'stranger', 'damn', 'sight', 'times', 'federation', 'life', 'wonder', 'together', 'looking', 'from', 'focus', 'totally', 'made', 'decided', 'mailbox', 'wait', 'complete', 'so', 'dinner', 'snow', 'up', 'great', 'when', 'boy', 'if', 'green', 'walker', 'fast', 'thing', 'of', 'hehe', 'eat', 'last', 'tragedy', 'meet', 'been', 'do', 'tired', 'are', 'national', 'know', 'its', 'come', 'wakes', 'all', 'reconnect', 'they', 'anyone', 'better', 'room', 'game', 'only', 'get', 'top', 'day', 'will', 'beat', 'way', 'in', 'shirt', 'rink', 'due', 'had', 'office', 'rapidly', 'field', 'special', 'is', 'move', 'smile', 'reason', 'street', 'go', 'some', 'tag', 'me', 'new', 'the', 'another', 'check', 'how', 'ice', 'love', 'perfect', 'billion', 'alchemy', 'desk', 'month', 'be', 'ever', 'it', 'week', 'for', 'on', 'd', 'see', 'am', 'movie', 'beauty', 'with', 'catch', 'by', 'we', 'no', 'fun', 'ford', 'talk', 'need', 'watch', 'gorgeous', 'work', 'sally', 'year', 'lewis', 'out', 'picture', 'have', 'long', 'commercial', 'soon', 'set', 'begin', 'according', 'and', 'going', 'more'}\n"
     ]
    }
   ],
   "source": [
    "# Fonction de prepare_data\n",
    "def prepare_data(infile, ngram_size, UNK_threshold=1):\n",
    "    # Initialiser word counts dictionary\n",
    "    word_counts = defaultdict(int)\n",
    "    # Lire le corpus\n",
    "    with open(infile, 'r', encoding='utf-8') as file:\n",
    "        corpus = file.read().lower()\n",
    "    # supprimer les tags htmls\n",
    "    corpus = re.sub(r'<.*?>', '', corpus)\n",
    "    # supprimer les URLS\n",
    "    corpus = re.sub(r'https?:?//\\S+', '', corpus)\n",
    "\n",
    "    # Fixer les contractions\n",
    "    expanded_corpus = contractions.fix(corpus)\n",
    "\n",
    "    # Tokenizer les phrases\n",
    "    sentences = expanded_corpus.split('\\n')\n",
    "\n",
    "    # Le vocabulaire d'anglais\n",
    "    english_vocab = set(w.lower() for w in nltk_words.words())\n",
    "\n",
    "    # Tokenizer les mots et compter leurs frequences\n",
    "    for sentence in sentences:\n",
    "        # Tokenizer la phrase en mots\n",
    "        sentence_tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "        # les frequences de mots\n",
    "        for token in sentence_tokens:\n",
    "            if token in english_vocab:\n",
    "                word_counts[token] += 1\n",
    "\n",
    "    # Collecter les mots qui vonts etre remplac√©es avec <UNK> et ajouter les mots dans le vocabulaire\n",
    "\n",
    "    words_to_replace = set()\n",
    "    vocab = set()\n",
    "    for word, count in word_counts.items():\n",
    "        if count < UNK_threshold:\n",
    "            words_to_replace.add(word)\n",
    "        else:\n",
    "            vocab.add(word)\n",
    "\n",
    "    # Replacer les mots with <UNK> and filtrer les mots non anglais\n",
    "    processed_tokens = []\n",
    "    for sentence in sentences:\n",
    "        # Tokenize the sentence into words\n",
    "        sentence_tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "        # Replace rare words with <UNK> and filter non-English words\n",
    "        sentence_tokens_with_unk = []\n",
    "        for token in sentence_tokens:\n",
    "            if token.isalpha():  # Check if the token is alphabetic\n",
    "               if token in vocab:  # Check if the token is in the vocabulary\n",
    "                  sentence_tokens_with_unk.append(token)\n",
    "               else:\n",
    "                  sentence_tokens_with_unk.append('<UNK>')  # Replace rare words with <UNK>\n",
    "            else:\n",
    "                sentence_tokens_with_unk.append(token)  # Preserve non-alphabetic characters\n",
    "        # Add sentence boundary markers\n",
    "        sentence_tokens_with_unk.insert(0, '<s> ')  # Add <s> tag at the beginning\n",
    "        sentence_tokens_with_unk.append(' </s>')    # Add </s> tag at the end\n",
    "\n",
    "        processed_tokens.append(' '.join(sentence_tokens_with_unk))\n",
    "\n",
    "    # Add an other  start token in the begining if ngram_size is 3\n",
    "    if ngram_size == 2:\n",
    "        pass\n",
    "    elif ngram_size == 3:\n",
    "        start_token = '<s> '  # Start token for trigram model\n",
    "        processed_tokens.insert(0, start_token)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported ngram_size. Only 2 and 3 are supported.\")\n",
    "    # Return preprocessed corpus\n",
    "    return ' '.join(processed_tokens), vocab,word_counts\n",
    "\n",
    "# Example usage\n",
    "infile = 'test.txt'\n",
    "preprocessed_corpus, vocab,word_counts = prepare_data(infile,2)\n",
    "print(\"Preprocessed corpus:\", preprocessed_corpus)\n",
    "print(\"vocab:\", vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NC68gcYcrT2"
   },
   "source": [
    "<a name='1.2'></a>\n",
    "2. La m√©thode train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pF3H05FYle0O"
   },
   "outputs": [],
   "source": [
    "def train(infile, ngram_size,k=1):\n",
    "    # Pr√©traitement du corpus\n",
    "    preprocessed_corpus, vocab, word_counts = prepare_data(infile, ngram_size)\n",
    "    bigram_counts = defaultdict(int)\n",
    "    trigram_counts = defaultdict(int)\n",
    "\n",
    "    # Calcul des fr√©quences des n-grammes\n",
    "    if ngram_size == 2:\n",
    "        ngram_counts = bigram_counts\n",
    "    elif ngram_size == 3:\n",
    "        ngram_counts = trigram_counts\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported ngram_size. Only 2 and 3 are supported.\")\n",
    "    #print(preprocessed_corpus)\n",
    "    tokens = preprocessed_corpus.split()\n",
    "\n",
    "    # Iterating over tokens to count n-grams\n",
    "    for i in range(len(tokens) - ngram_size + 1):\n",
    "        ngram = tuple(tokens[i:i + ngram_size])\n",
    "        ngram_counts[ngram] += 1\n",
    "    vocabulary=set(tokens) # Contient tous les diffrents tockens mais cette foi ci avec les tags <s> et </s>\n",
    "    # Lissage add-k\n",
    "    #print(vocabulary)\n",
    "    vocab_size = len(vocabulary)-2 # Exclure les tags <s> et </s>\n",
    "    #print(vocab_size)\n",
    "    for ngram, count in ngram_counts.items():\n",
    "        denominator = word_counts[ngram[:-1]] + (k * vocab_size)\n",
    "        ngram_counts[ngram] = (count + k) / denominator\n",
    "\n",
    "    # Transformation des probabilit√©s en logarithmes\n",
    "    for ngram, probability in ngram_counts.items():\n",
    "        ngram_counts[ngram] = math.log(probability)\n",
    "\n",
    "    return ngram_counts,vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Istu9Gd0CitO"
   },
   "outputs": [],
   "source": [
    "bigram_probabilities=train('ngramv1.test',2)\n",
    "#print(\"bigram_probabilities\",bigram_probabilities)\n",
    "trigram_probabilities=train('ngramv1.test',3)\n",
    "#print(\"trigram_probabilities\",trigram_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xugC1Rq9PnJs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8M6UxG32Jlrl"
   },
   "source": [
    "<a name='1.3'></a>\n",
    "3. La methode predict_ngram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60kIeZYRJcJg"
   },
   "outputs": [],
   "source": [
    "def predict_ngram(sentence, ngram_size):\n",
    "        file_path = \"sentence.txt\"\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "             file.write(sentence)\n",
    "        probabilities,vocab=train(file_path,ngram_size)\n",
    "        probability = sum(probabilities.values())\n",
    "        #print(\"La probabilit√© de la phrase est: \")\n",
    "        return probability\n",
    "\n",
    "# La probabilite calcul√© ici est la somme des logs des probabilites de chaque probabilite p(wi/wi-1) ou p(wi/wi-1wi-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1gmdVuVNLE9"
   },
   "source": [
    "Test de fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_T-rU4NVnas",
    "outputId": "0939f59e-0fc9-4706-ddf7-1cfda6fc1734"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.872607577470724"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage:\n",
    "sentence = \"This is a sample sentence !!!!!\"\n",
    "predict_ngram(sentence,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5HcilRoJu8q"
   },
   "source": [
    "4. La methode test_perplexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LpAu-urcZOKu",
    "outputId": "528541d1-92d8-472e-a01c-5b2bdee45cdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "say ! \n",
      "i like green eggs and ham !\n",
      "i do ! \n",
      "i like them, sam i am !\n",
      "and i would eat them in a boat .\n",
      "and i would eat them with a goat .\n",
      "and i will eat them, in the rain .\n",
      "and in the dark .\n",
      "and on a train .\n",
      "and in a car .\n",
      "and in a tree .\n",
      "they are so good, so good, you see !\n",
      "so i will eat them in a box .\n",
      "and i will eat them with a fox .\n",
      "and i will eat them in a house .\n",
      "and i will eat them with a mouse .\n",
      "and i will eat them here and there .\n",
      "say ! \n",
      "i will eat them anywhere !\n",
      "i do so like green eggs and ham !\n",
      "thank you !\n",
      "thank you, sam i am .\n",
      "say ! \n",
      "i like green eggs and ham !\n",
      "i do ! \n",
      "i like them, sam i am !\n",
      "and i would eat them in a boat .\n",
      "and i would eat them with a goat .\n",
      "and i will eat them, in the rain .\n",
      "and in the dark .\n",
      "and on a train .\n",
      "and in a car .\n",
      "and in a tree .\n",
      "they are so good, so good, you see !\n",
      "so i will eat them in a box .\n",
      "and i will eat them with a fox .\n",
      "and i will eat them in a house .\n",
      "and i will eat them with a mouse .\n",
      "and i will eat them here and there .\n",
      "say ! \n",
      "i will eat them anywhere !\n",
      "i do so like green eggs and ham !\n",
      "thank you !\n",
      "thank you, sam i am .\n",
      "Perplexity: 3.5775992222494146\n",
      "say ! \n",
      "i like green eggs and ham !\n",
      "i do ! \n",
      "i like them, sam i am !\n",
      "and i would eat them in a boat .\n",
      "and i would eat them with a goat .\n",
      "and i will eat them, in the rain .\n",
      "and in the dark .\n",
      "and on a train .\n",
      "and in a car .\n",
      "and in a tree .\n",
      "they are so good, so good, you see !\n",
      "so i will eat them in a box .\n",
      "and i will eat them with a fox .\n",
      "and i will eat them in a house .\n",
      "and i will eat them with a mouse .\n",
      "and i will eat them here and there .\n",
      "say ! \n",
      "i will eat them anywhere !\n",
      "i do so like green eggs and ham !\n",
      "thank you !\n",
      "thank you, sam i am .\n",
      "say ! \n",
      "i like green eggs and ham !\n",
      "i do ! \n",
      "i like them, sam i am !\n",
      "and i would eat them in a boat .\n",
      "and i would eat them with a goat .\n",
      "and i will eat them, in the rain .\n",
      "and in the dark .\n",
      "and on a train .\n",
      "and in a car .\n",
      "and in a tree .\n",
      "they are so good, so good, you see !\n",
      "so i will eat them in a box .\n",
      "and i will eat them with a fox .\n",
      "and i will eat them in a house .\n",
      "and i will eat them with a mouse .\n",
      "and i will eat them here and there .\n",
      "say ! \n",
      "i will eat them anywhere !\n",
      "i do so like green eggs and ham !\n",
      "thank you !\n",
      "thank you, sam i am .\n",
      "Perplexity: 3.6159593547612707\n"
     ]
    }
   ],
   "source": [
    "def test_perplexity(test_file, ngram_size=2):\n",
    "        with open(test_file, 'r', encoding='utf-8') as file:\n",
    "            test_corpus = file.read().lower()\n",
    "        print(test_corpus)\n",
    "        # Initialisation de la perplexit√©\n",
    "        total_log_prob = 0\n",
    "        total_words = 0\n",
    "\n",
    "        # Calcul de la perplexit√© pour chaque phrase dans le corpus de test\n",
    "        for sentence in test_corpus.split('\\n'):\n",
    "           if sentence:\n",
    "                print(sentence)\n",
    "                # Calcul de la probabilit√© de la phrase\n",
    "                log_prob = predict_ngram(sentence, ngram_size)\n",
    "                total_log_prob += log_prob\n",
    "                total_words += len(sentence.split())+1 # On ajoute 1 pour chaque phrase en comptant la tage </s>\n",
    "\n",
    "\n",
    "        # Calcul de la perplexit√©\n",
    "        perplexity = math.exp(total_log_prob)**(-1/float(total_words))\n",
    "        return perplexity\n",
    "\n",
    "# Exemple d'utilisation :\n",
    "perplexity_1 = test_perplexity(test_file=\"ngramv1.test\", ngram_size=2)\n",
    "print(\"Perplexity:\", perplexity_1)\n",
    "\n",
    "# Exemple d'utilisation :\n",
    "perplexity_2 = test_perplexity(test_file=\"ngramv1.test\", ngram_size=3)\n",
    "print(\"Perplexity:\", perplexity_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciEFtQdlwnR8"
   },
   "source": [
    "<a name='2'></a>\n",
    "## **II. PART two:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NNQO99VxUCW"
   },
   "source": [
    "<a name='2.1'></a>\n",
    "2.1. Preprocessing et l'entrainment de mod√®le sur les donn√©es de fichier bigdata.txt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vqvrv62pxU-i"
   },
   "outputs": [],
   "source": [
    "bigram_probabilities,vocab=train('big_data.txt',2)\n",
    "#print(\"bigram_probabilities\",bigram_probabilities)\n",
    "trigram_probabilities,vocab=train('big_data.txt',3)\n",
    "#print(\"trigram_probabilities\",trigram_probabilities)\n",
    "\n",
    "#Ici concernant le  vocabulaire contient les memes mots pour les deux mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SUmcfv4JL2Mi"
   },
   "outputs": [],
   "source": [
    "def save_probabilities_to_file(probabilities, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for ngram, prob in probabilities.items():\n",
    "            file.write(f'{ngram}: {prob}\\n')\n",
    "\n",
    "\n",
    "save_probabilities_to_file(bigram_probabilities, 'bigram_probabilities.txt')\n",
    "save_probabilities_to_file(trigram_probabilities, 'trigram_probabilities.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UIuiqzvWL2BP"
   },
   "outputs": [],
   "source": [
    "def save_vocab_to_file(vocab, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for word in vocab:\n",
    "            file.write(f'{word}\\n')\n",
    "\n",
    "# Assuming bigram_vocab and trigram_vocab are lists containing vocabularies\n",
    "save_vocab_to_file(vocab, 'big_data_vocab.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mp5R_9uwHZL3"
   },
   "source": [
    "<a name='2.2'></a>\n",
    "2.2 La methode generate_text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FTZ37BCkQFcK",
    "outputId": "63cd8a0e-35ae-4302-e633-ac793d641314"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "a=(1,2,3)\n",
    "print(a[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dC91X7cRQ7MH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dSe9gZfNL1xo"
   },
   "outputs": [],
   "source": [
    "def generate_text(trained_model, vocab, ngram_size, max_length=100):\n",
    "    current_token_bigram = (\"<s> \",)\n",
    "    current_token_trigram = (\"<s> \",\"<s> \",)  # Ensure that the current token is a tuple\n",
    "    generated_text = []\n",
    "\n",
    "    while current_token_bigram != (\"</s>\",) or current_token_trigram!=  (\"</s>\",) and len(generated_text) < max_length:\n",
    "        if ngram_size == 2:  # bigram model\n",
    "            next_token_probs = {\n",
    "                token[1]: np.exp(prob) for token, prob in trained_model.items() if token[0] == current_token_bigram[0]\n",
    "            }\n",
    "        elif ngram_size == 3:  # trigram model\n",
    "            next_token_probs = {\n",
    "                token[2]: np.exp(prob) for token, prob in trained_model.items() if token[:2] == current_token_trigram[:2]\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported ngram_size. Only 2 and 3 are supported.\")\n",
    "\n",
    "        if not next_token_probs:\n",
    "            print(\"No next word found, stopping generation.\")\n",
    "            break  # Stop generation if no next word found\n",
    "\n",
    "        # Exclude \"<UNK>\" token from next_token_probs\n",
    "        next_token_probs = {token: prob for token, prob in next_token_probs.items() if token != \"<UNK>\" }\n",
    "\n",
    "        # Normalize probabilities\n",
    "        total_prob = sum(next_token_probs.values())\n",
    "        next_token_probs = {token: prob / total_prob for token, prob in next_token_probs.items()}\n",
    "\n",
    "        # Sample the next token proportionally to its probabilities\n",
    "        next_token = np.random.choice(list(next_token_probs.keys()), p=list(next_token_probs.values()))\n",
    "        generated_text.append(next_token)\n",
    "\n",
    "        # Update the current token\n",
    "        if ngram_size==2:\n",
    "           current_token_bigram= current_token_bigram[1:] + (next_token,)\n",
    "        if ngram_size==3:\n",
    "           current_token_trigram= current_token_trigram[1:] + (next_token,)\n",
    "\n",
    "    return \" \".join(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KKw1qTeNTf2R"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_text(trained_model, vocab, ngram_size, max_length=100):\n",
    "    current_token_bigram = (\"<s>\",)\n",
    "    current_token_trigram = (\"<s>\", \"<s>\")\n",
    "    generated_text = []\n",
    "\n",
    "    while (ngram_size == 2 and current_token_bigram != (\"</s>\",) or ngram_size == 3 and current_token_trigram != (\"</s>\",)) and len(generated_text) < max_length:\n",
    "        if ngram_size == 2:  # bigram model\n",
    "            next_token_probs = {\n",
    "                token[1]: np.exp(prob) for token, prob in trained_model.items() if token[0] == current_token_bigram[0]\n",
    "            }\n",
    "        elif ngram_size == 3:  # trigram model\n",
    "            next_token_probs = {\n",
    "                token[2]: np.exp(prob) for token, prob in trained_model.items() if token[:2] == current_token_trigram[:2]\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported ngram_size. Only 2 and 3 are supported.\")\n",
    "\n",
    "        if not next_token_probs:\n",
    "            print(f\"No next word found for context \")\n",
    "            break  # Stop generation if no next word found\n",
    "\n",
    "        # Exclude \"<UNK>\" token from next_token_probs\n",
    "        next_token_probs = {token: prob for token, prob in next_token_probs.items() if token != \"<UNK>\"}\n",
    "\n",
    "        if not next_token_probs:\n",
    "           print(f\"All next words are <UNK> for context\")\n",
    "           break  # Stop generation if all next words are <UNK>\n",
    "\n",
    "        # Normalize probabilities\n",
    "        total_prob = sum(next_token_probs.values())\n",
    "        next_token_probs = {token: prob / total_prob for token, prob in next_token_probs.items()}\n",
    "\n",
    "        # Sample the next token proportionally to its probabilities\n",
    "        next_token = np.random.choice(list(next_token_probs.keys()), p=list(next_token_probs.values()))\n",
    "        generated_text.append(next_token)\n",
    "\n",
    "        # Update the current token\n",
    "        if ngram_size == 2:\n",
    "            current_token_bigram = (next_token,)\n",
    "        elif ngram_size == 3:\n",
    "            current_token_trigram = (current_token_trigram[1], next_token)\n",
    "\n",
    "    return \" \".join(generated_text)\n",
    "\n",
    "# Example usage\n",
    "# trained_model, vocab, and ngram_size would be defined elsewhere in your code\n",
    "# generated_text = generate_text(trained_model, vocab, ngram_size)\n",
    "# print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NZPdk_-OIPhV",
    "outputId": "4595e624-d574-4df2-bd02-4434eb4b6b24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All next words are <UNK> for context\n",
      "Texte g√©n√©r√© avec un mod√®le bigramme :\n",
      " my america is a loaner\n"
     ]
    }
   ],
   "source": [
    "generated_text_bigram = generate_text(bigram_probabilities, vocab, ngram_size=2, max_length=100)\n",
    "print(\"Texte g√©n√©r√© avec un mod√®le bigramme :\\n\", generated_text_bigram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UxnSKI_7T1uu",
    "outputId": "df8018df-de80-4db0-ae58-88ab31be712b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All next words are <UNK> for context\n",
      "Texte g√©n√©r√© avec un mod√®le trigramme :\n",
      " how come everyone\n"
     ]
    }
   ],
   "source": [
    "generated_text_trigram = generate_text(trigram_probabilities, vocab, ngram_size=3, max_length=100)\n",
    "print(\"Texte g√©n√©r√© avec un mod√®le trigramme :\\n\", generated_text_trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLGMskrqy_sa"
   },
   "source": [
    "<a name='2.3'></a>\n",
    "2.3. La m√©thode autoComplete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5q5beWsXJpP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s36jBDCFAypT",
    "outputId": "f13db476-90d1-4db1-ab57-9ea618d1bf43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All next words are <UNK> for context\n",
      "Next word after 'i want to know  the truth that': you\n"
     ]
    }
   ],
   "source": [
    "def autoComplete(text, ngram_size, bigram_prob, trigram_prob, vocab):\n",
    "    # Load n-gram probabilities\n",
    "    if ngram_size == 2:\n",
    "        probabilities = bigram_prob\n",
    "    elif ngram_size == 3:\n",
    "        probabilities = trigram_prob\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported ngram_size. Only 2 and 3 are supported.\")\n",
    "\n",
    "    # Tokenize input text\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Extract last (n-1) tokens as the current token\n",
    "    current_token = tuple(tokens[-(ngram_size-1):])\n",
    "\n",
    "    # Generate auto-completion\n",
    "    generated_text = generate_text(probabilities, vocab, ngram_size)\n",
    "\n",
    "    # Extract the next word from generated text\n",
    "    next_word = None\n",
    "    if generated_text:\n",
    "        generated_tokens = generated_text.split()\n",
    "        if len(generated_tokens) >= ngram_size:\n",
    "            next_word = generated_tokens[ngram_size-1]\n",
    "        else:\n",
    "            print(\"Generated text does not contain enough tokens.\")\n",
    "    else:\n",
    "        print(\"No text generated.\")\n",
    "\n",
    "    return next_word\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    ngram_size = 3\n",
    "    text = \"i want to know  the truth that\"\n",
    "    next_word = autoComplete(text, ngram_size, bigram_probabilities, trigram_probabilities, vocab)\n",
    "    print(\"Next word after '{}': {}\".format(text, next_word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvXitgnW0dRF"
   },
   "source": [
    "<a name='2.4'></a>\n",
    "2.4. La m√©thode de correction: (TP1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ct_buSTzy-64",
    "outputId": "f4185403-b6af-48ee-dc18-c5d884cfdc95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates for 'iant': [('want', 0.0002895341905816231), ('ant', 2.6891720487457255e-06), ('giant', 2.3306157755796287e-05)]\n"
     ]
    }
   ],
   "source": [
    "def build_language_model(corpus_file):\n",
    "    # Pr√©traitement du texte du corpus\n",
    "    with open(corpus_file, 'r', encoding='utf-8') as file:\n",
    "        text = file.read().lower()\n",
    "\n",
    "    words = re.findall(r'\\w+', text)\n",
    "\n",
    "    # Comptage du nombre d'occurrences de chaque mot\n",
    "    word_counts = Counter(words)\n",
    "    total_words = len(words)\n",
    "\n",
    "    # Calcul des probabilit√©s de chaque mot\n",
    "    language_model = {word: count / total_words for word, count in word_counts.items()}\n",
    "\n",
    "    return language_model\n",
    "\n",
    "def edits1(word):\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes = [left + right[1:] for left, right in splits if right]\n",
    "    transposes = [left + right[1] + right[0] + right[2:] for left, right in splits if len(right) > 1]\n",
    "    replaces = [left + c + right[1:] for left, right in splits if right for c in alphabet]\n",
    "    inserts = [left + c + right for left, right in splits for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word):\n",
    "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "def known_words(words, vocabulary):\n",
    "    return set(word for word in words if word in vocabulary)\n",
    "\n",
    "def candidates(word, vocabulary, language_model):\n",
    "    # Si le mot original est dans le vocabulaire, retournez-le comme seul candidat avec sa probabilit√©\n",
    "    if word in language_model:\n",
    "        return [(word, language_model[word])]\n",
    "\n",
    "    # Recherchez les mots connus √† une distance d'√©dition de un\n",
    "    candidates_1 = known_words(edits1(word), vocabulary)\n",
    "    candidates_1_with_prob = [(c, language_model.get(c, 0)) for c in candidates_1]\n",
    "    if candidates_1_with_prob:\n",
    "        return candidates_1_with_prob\n",
    "\n",
    "    # Recherchez les mots connus √† une distance d'√©dition de deux\n",
    "    candidates_2 = known_words(edits2(word), vocabulary)\n",
    "    candidates_2_with_prob = [(c, language_model.get(c, 0)) for c in candidates_2]\n",
    "    if candidates_2_with_prob:\n",
    "        return candidates_2_with_prob\n",
    "\n",
    "    # Si aucune liste de candidats n'est trouv√©e, retournez simplement le mot original avec une probabilit√© nulle\n",
    "    return [(word, 0)]\n",
    "\n",
    "# Fonction pour construire un mod√®le de langue bigramme\n",
    "def build_bigram_language_model(corpus):\n",
    "    # Concat√©ner tous les mots du corpus\n",
    "    all_words = ' '.join(corpus.keys())\n",
    "    words = all_words.split()\n",
    "\n",
    "    # G√©n√©rer les paires de mots cons√©cutifs pour les bigrammes\n",
    "    bigrams = [(words[i], words[i+1]) for i in range(len(words)-1)]\n",
    "\n",
    "    # Compter les occurrences de chaque bigramme\n",
    "    bigram_counts = Counter(bigrams)\n",
    "    total_bigrams = sum(bigram_counts.values())\n",
    "\n",
    "    # Calculer les probabilit√©s de chaque bigramme\n",
    "    bigram_probabilities = {bigram: count / total_bigrams for bigram, count in bigram_counts.items()}\n",
    "\n",
    "    return bigram_probabilities\n",
    "\n",
    "\n",
    "# Fonction pour calculer la probabilit√© d'un candidat en utilisant le mod√®le de langue bigramme\n",
    "def calculate_probability(candidate, bigram_model):\n",
    "    word1, word2 = candidate\n",
    "    bigram = (word1, word2)\n",
    "    return bigram_model.get(bigram, 0)\n",
    "\n",
    "# Fonction pour ordonner les candidats par probabilit√©\n",
    "def order_candidates_by_probability(candidates, bigram_model):\n",
    "    # Triez les candidats en fonction de leurs probabilit√©s calcul√©es\n",
    "    ordered_candidates = sorted(candidates, key=lambda x: calculate_probability(x, bigram_model), reverse=True)\n",
    "    return ordered_candidates\n",
    "\n",
    "# Fonction principale pour le correcteur orthographique utilisant un mod√®le de langue bigramme\n",
    "def spell_corrector_bigram(word, vocabulary, language_model, bigram_model):\n",
    "    # G√©n√©ration des candidats\n",
    "    candidates_list = candidates(word, vocabulary, language_model)\n",
    "\n",
    "    # Ordonner les candidats par probabilit√© en utilisant le mod√®le de langue bigramme\n",
    "    ordered_candidates = order_candidates_by_probability(candidates_list, bigram_model)\n",
    "\n",
    "    return ordered_candidates\n",
    "\n",
    "# Exemple d'utilisation\n",
    "corpus_file = 'big.txt'\n",
    "language_model = build_language_model(corpus_file)\n",
    "vocabulary = set(language_model.keys())\n",
    "# Utilisation de la fonction build_bigram_language_model avec language_model\n",
    "bigram_model = build_bigram_language_model(language_model)\n",
    "\n",
    "word = \"iant\"\n",
    "print(\"Candidates for 'iant':\", spell_corrector_bigram(word, vocabulary, language_model, bigram_model))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdL8hIa7y872"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oI7UifnoyxJH"
   },
   "source": [
    "<a name='3'></a>\n",
    "## **III. PART three:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h6dN8MZgNwjB",
    "outputId": "7ca687eb-2837-407b-f880-32025a94bbc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting conllu\n",
      "  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
      "Installing collected packages: conllu\n",
      "Successfully installed conllu-4.5.3\n"
     ]
    }
   ],
   "source": [
    "!pip install conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3M_B3BORYVGF"
   },
   "outputs": [],
   "source": [
    "from conllu import parse_incr\n",
    "from io import open\n",
    "from sys import argv\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_corpus(lang):\n",
    "\treturn 'fr_rhapsodie-ud-train.conllu'\n",
    "\n",
    "\n",
    "def test_corpus(lang):\n",
    "\treturn 'fr_rhapsodie-ud-test.conllu'\n",
    "\n",
    "\n",
    "def prune_sentence(sent):\n",
    "\treturn [token for token in sent if type(token['id']) is int]\n",
    "\n",
    "\n",
    "def conllu_corpus(path):\n",
    "\tdata_file = open(path, 'r', encoding='utf-8')\n",
    "\tsents = list(parse_incr(data_file))\n",
    "\treturn [prune_sentence(sent) for sent in sents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfmbIEdZYsVx"
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist, WittenBellProbDist, bigrams\n",
    "\n",
    "START = '<s>'\n",
    "END = '</s>'\n",
    "BINS = 1e5\n",
    "\n",
    "\n",
    "def get_smoothed(mappings):\n",
    "\t# Initialize the smoothed matrix\n",
    "\tsmoothed = {}\n",
    "\ttags = set([t for (t,_) in mappings])\n",
    "\tfor tag in tags:\n",
    "\t\twords = [w for (t,w) in mappings if t == tag]\n",
    "\t\t# Smooth the mapping using Witten-Bell smoothing\n",
    "\t\tsmoothed[tag] = WittenBellProbDist(FreqDist(words), bins=BINS)\n",
    "\treturn smoothed\n",
    "\n",
    "\n",
    "def get_transmission_prob_matrix(train_sents):\n",
    "\t# Initialize the transition matrix\n",
    "\ttransitions = []\n",
    "\n",
    "\tfor sent in train_sents:\n",
    "\t\t# Add the first word of each sentence to the START tag\n",
    "\t\ttransitions.append((START, sent[0]['upos']))\n",
    "\n",
    "\t\t# Add the transition from the previous word to the current word\n",
    "\t\tfor token in bigrams(sent):\n",
    "\t\t\ttransitions.append((token[0]['upos'], token[1]['upos']))\n",
    "\n",
    "\t\t# Add the transition from the last word to the END tag\n",
    "\t\ttransitions.append((sent[len(sent) - 1]['upos'], END))\n",
    "\n",
    "\t# Return the smoothed transition matrix\n",
    "\treturn get_smoothed(transitions)\n",
    "\n",
    "def get_emission_prob_matrix(train_sents):\n",
    "\t# Initialize the emission matrix\n",
    "    emissions = [(token['upos'], token['form']) for sentence in train_sents for token in sentence]\n",
    "\n",
    "\t# Return the smoothed emission matrix\n",
    "    return get_smoothed(emissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "isMEHTccY4WY"
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "\n",
    "def viterbi_algorithm(tags, emission_probs, transition_probs, test_sentence):\n",
    "\t# Initialize the Viterbi matrix and backpointers\n",
    "\tV = [{}]\n",
    "\tbackpointers = {}\n",
    "\n",
    "\t# Initialize the first row of the Viterbi matrix\n",
    "\tfor tag in tags:\n",
    "\t\tV[0][tag] = log(transition_probs[START].prob(tag)) + log(emission_probs[tag].prob(test_sentence[0]['form']))\n",
    "\t\tbackpointers[tag] = [tag]\n",
    "\n",
    "\t# Fill in the rest of the Viterbi matrix\n",
    "\tfor word_index in range(1,len(test_sentence)):\n",
    "\t\tV.append({})\n",
    "\t\ttemp_backpointers = {}\n",
    "\n",
    "\t\t# Find the maximum probability for each tag\n",
    "\t\tfor curr_tag in tags:\n",
    "\t\t\tall_probs = []\n",
    "\t\t\tfor prev_tag in tags:\n",
    "\t\t\t\tprob = V[word_index - 1][prev_tag] + log(transition_probs[prev_tag].prob(curr_tag)) + log(emission_probs[curr_tag].prob(test_sentence[word_index]['form']))\n",
    "\t\t\t\tall_probs.append((prob, prev_tag))\n",
    "\t\t\tmax_prob, prev_tag = max(all_probs)\n",
    "\n",
    "\t\t\t# Add the maximum probability to the Viterbi matrix and the backpointer to the temp backpointers\n",
    "\t\t\tV[word_index][curr_tag] = max_prob\n",
    "\t\t\ttemp_backpointers[curr_tag] = backpointers[prev_tag] + [curr_tag]\n",
    "\n",
    "\t\t# Update the backpointers\n",
    "\t\tbackpointers = temp_backpointers\n",
    "\n",
    "\t# Find the maximum probability for the END tag\n",
    "\tmax_prob, end_tag = max([(V[-1][tag] + log(transition_probs[tag].prob(END)), tag) for tag in tags])\n",
    "\n",
    "\t# Return the most probable sequence of tags\n",
    "\treturn backpointers[end_tag]\n",
    "\n",
    "\n",
    "def get_accuracy(tags, emission_matrix, transition_matrix, test_sents):\n",
    "\tcorrect = 0\n",
    "\ttotal = 0\n",
    "\n",
    "\tfor sent in test_sents:\n",
    "\t\tpredicted_tags = viterbi_algorithm(tags, emission_matrix, transition_matrix, sent)\n",
    "\t\t# Get the accuracy for each word in the test sentence based on the predicted tag\n",
    "\t\tfor token_index, token in enumerate(sent):\n",
    "\t\t\tif token['upos'] == predicted_tags[token_index]:\n",
    "\t\t\t\tcorrect += 1\n",
    "\t\t\ttotal += 1\n",
    "\treturn correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ifltM4AfY7r9"
   },
   "outputs": [],
   "source": [
    "\n",
    "def find_accuracy(lang):\n",
    "\ttrain_sents = conllu_corpus(train_corpus(lang))\n",
    "\ttest_sents = conllu_corpus(test_corpus(lang))\n",
    "\ttags = set([token['upos'] for sent in train_sents for token in sent])\n",
    "\n",
    "\t# Get the transition and emission matrices\n",
    "\ttransition_matrix = get_transmission_prob_matrix(train_sents)\n",
    "\temission_matrix = get_emission_prob_matrix(train_sents)\n",
    "\n",
    "\t# Get the accuracy of the model using the Viertbi algorithm\n",
    "\taccuracy = get_accuracy(tags, emission_matrix, transition_matrix, test_sents)\n",
    "\n",
    "\n",
    "\t# Print the accuracy\n",
    "\tmatch lang:\n",
    "\t\tcase 'en':\n",
    "\t\t\tprint('English Accuracy: {:.2%}'.format(accuracy))\n",
    "\t\tcase 'fr':\n",
    "\t\t\tprint('French Accuracy: {:.2%}'.format(accuracy))\n",
    "\t\tcase 'uk':\n",
    "\t\t\tprint('Ukrainian Accuracy: {:.2%}'.format(accuracy))\n",
    "\t\tcase _:\n",
    "\t\t\tprint('Accuracy: {:.2%}'.format(accuracy))\n",
    "\n",
    "\treturn accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JX0aH0uUZG3n",
    "outputId": "3c94bdd3-1926-40b7-a501-a2864a045df2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Accuracy: 91.78%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9178082191780822"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_accuracy('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FReMl4LTZTLo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
